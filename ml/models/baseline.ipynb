{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eb02d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from langfuse import Langfuse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"C:/vuzik/sii/bachelor-2025-team-losoci/ml\"))\n",
    "\n",
    "import prompt_templates\n",
    "from prompt_templates import ProfileText\n",
    "from preprocessing_pipeline import DataPreprocessor\n",
    "\n",
    "from langfuse import Langfuse\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce40eacb",
   "metadata": {},
   "source": [
    "### Langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7906bd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse = Langfuse(\n",
    "        secret_key=os.getenv('LANGFUSE_SK'),\n",
    "        public_key=os.getenv('LANGFUSE_PK'),\n",
    "        host=os.getenv('LANGFUSE_HOST')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d665f2e8",
   "metadata": {},
   "source": [
    "#### Курс доллара\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12f58889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_currency_rate():\n",
    "    \n",
    "    url = \"https://api.exchangerate-api.com/v4/latest/USD\"\n",
    "    #try:\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    rub_rate = data['rates']['RUB']\n",
    "    return rub_rate\n",
    "    #except Exception as e:\n",
    "        #print(f\"Ошибка при получении курса: {e}\")\n",
    "        #return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df76bcb3",
   "metadata": {},
   "source": [
    "### Запросы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ebbeaf",
   "metadata": {},
   "source": [
    "#### По профилю"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4376c61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.checkpoint.memory import InMemorySaver  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad2643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    validate_model_on_init=True,\n",
    "    temperature=0.4,\n",
    "    max_tokens=2048,\n",
    ")\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "pt = ProfileText()\n",
    "\n",
    "chat_agent = create_agent(\n",
    "    model,\n",
    "    system_prompt=pt.system_prompt,\n",
    "    checkpointer=checkpointer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61172521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt: str, \n",
    "                     system: str,\n",
    "                     model_name: str = None,\n",
    "                     max_tokens: int = 500,\n",
    "                     temperature: float = 0.7):\n",
    "                     \n",
    "    model = model_name or os.getenv('MODEL_NAME')\n",
    "    system_content = system\n",
    "\n",
    "    with langfuse.start_as_current_generation(\n",
    "        name=f\"Dataset test: pfp_1\",\n",
    "        model=model,\n",
    "        input={\n",
    "            \"system\": system_content,\n",
    "            \"prompt\": prompt},\n",
    "        model_parameters={\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens\n",
    "        }\n",
    "    ) as generation:\n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"system\": system_content,\n",
    "            \"stream\": False,\n",
    "            \"options\": {\n",
    "                \"temperature\": temperature,\n",
    "                \"max_tokens\": max_tokens\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        response = requests.post(os.getenv('BASE_URL'), json=data)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            full_response = response.json()\n",
    "            generated_text = full_response[\"response\"]\n",
    "\n",
    "            # Подсчет токенов\n",
    "            prompt_tokens = full_response.get(\"prompt_eval_count\", 0)\n",
    "            completion_tokens = full_response.get(\"eval_count\", 0)\n",
    "\n",
    "            # Подсчет стоимости токена из расчета\n",
    "            # rate = get_currency_rate()\n",
    "            currency_rate = 83\n",
    "            input_cost_per_token = 0.00000244 * currency_rate\n",
    "            output_cost_per_token = input_cost_per_token * 2.5 \n",
    "            \n",
    "            input_cost = prompt_tokens * input_cost_per_token\n",
    "            output_cost = completion_tokens * output_cost_per_token\n",
    "            total_cost = input_cost + output_cost\n",
    "\n",
    "            \n",
    "\n",
    "            generation.update(\n",
    "                output=generated_text,\n",
    "                usage_details={\n",
    "                    \"input\": prompt_tokens,\n",
    "                    \"output\": completion_tokens,\n",
    "                },\n",
    "                cost_details={\n",
    "                    \"input\": input_cost,\n",
    "                    \"output\": output_cost,\n",
    "                    \"total\": total_cost \n",
    "                },\n",
    "                metadata={\n",
    "                    \"response_time\": full_response.get(\"total_duration\", 0),\n",
    "                    \"model\": model,\n",
    "                    \"system\": system_content,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"stream\": False,\n",
    "                    #\"judge_response\": generated_judge_text,\n",
    "                    \"options\": {\n",
    "                        \"temperature\": temperature,\n",
    "                        \"max_tokens\": max_tokens\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "            )\n",
    "            \n",
    "            return generated_text\n",
    "            \n",
    "        else:\n",
    "            error_msg = f\"Ошибка API: {response.status_code} - {response.text}\"\n",
    "            \n",
    "            generation.update(\n",
    "                output={\"error\": error_msg},\n",
    "                level=\"ERROR\"\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": error_msg,\n",
    "                \"model\": model\n",
    "            }\n",
    "            \n",
    "    langfuse.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ff502f",
   "metadata": {},
   "source": [
    "#### Для локальной Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5437ed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_local_ollama(prompt: str, \n",
    "                     system: str,\n",
    "                     model_name: str = None,\n",
    "                     max_tokens: int = 500,\n",
    "                     temperature: float = 0.7):\n",
    "    \n",
    "    model = model_name or os.getenv('MODEL_NAME')\n",
    "    ollama_host = os.getenv('OLLAMA_HOST', 'http://localhost:11434')\n",
    "    \n",
    "    with langfuse.start_as_current_generation(\n",
    "        name=f\"Dataset test: pfp_1\",\n",
    "        model=model,\n",
    "        input={\n",
    "            \"system\": system,\n",
    "            \"prompt\": prompt},\n",
    "        model_parameters={\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens\n",
    "        }\n",
    "    ) as generation:\n",
    "        \n",
    "        try:\n",
    "            # Инициализация ChatOllama\n",
    "            llm = ChatOllama(\n",
    "                model=model,\n",
    "                temperature=temperature,\n",
    "                num_predict=max_tokens\n",
    "            )\n",
    "            \n",
    "            # Создание сообщений\n",
    "            messages = [\n",
    "                (\"system\", system),\n",
    "                (\"human\", prompt)\n",
    "            ]\n",
    "            \n",
    "            # Вызов модели\n",
    "            response = llm.invoke(messages) # response = requests.post(os.getenv('MODEL_NAME'), json=data)\n",
    "\n",
    "            generated_text = response.сcontent\n",
    "            \n",
    "            # Для получения точного количества токенов можно использовать дополнительный запрос\n",
    "            import requests\n",
    "            token_data = {\n",
    "                \"model\": model,\n",
    "                \"prompt\": f\"{system}\\n\\n{prompt}\"\n",
    "            }\n",
    "            \n",
    "            token_response = requests.post(f\"{ollama_host}/api/encode\", json=token_data)\n",
    "            if token_response.status_code == 200:\n",
    "                prompt_tokens = len(token_response.json().get(\"tokens\", []))\n",
    "            else:\n",
    "                prompt_tokens = len(prompt.split()) // 0.75\n",
    "                \n",
    "            completion_tokens = len(generated_text.split()) // 0.75\n",
    "            \n",
    "            # Подсчет стоимости\n",
    "            currency_rate = 83\n",
    "            input_cost_per_token = 0.00000244 * currency_rate\n",
    "            output_cost_per_token = input_cost_per_token * 2.5 \n",
    "            \n",
    "            input_cost = prompt_tokens * input_cost_per_token\n",
    "            output_cost = completion_tokens * output_cost_per_token\n",
    "            total_cost = input_cost + output_cost\n",
    "\n",
    "            generation.update(\n",
    "                output=generated_text,\n",
    "                usage_details={\n",
    "                    \"input\": prompt_tokens,\n",
    "                    \"output\": completion_tokens,\n",
    "                },\n",
    "                cost_details={\n",
    "                    \"input\": input_cost,\n",
    "                    \"output\": output_cost,\n",
    "                    \"total\": total_cost \n",
    "                },\n",
    "                metadata={\n",
    "                    \"model\": model,\n",
    "                    \"system\": system,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"temperature\": temperature,\n",
    "                    \"max_tokens\": max_tokens\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            return generated_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Ошибка ChatOllama: {str(e)}\"\n",
    "            \n",
    "            generation.update(\n",
    "                output={\"error\": error_msg},\n",
    "                level=\"ERROR\"\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": error_msg,\n",
    "                \"model\": model\n",
    "            }\n",
    "            \n",
    "    langfuse.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2353c17a",
   "metadata": {},
   "source": [
    "### Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76d9bf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.checkpoint.memory import InMemorySaver  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24cf0f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    validate_model_on_init=True,\n",
    "    temperature=0.4,\n",
    "    max_tokens=2048,\n",
    ")\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "pt = ProfileText()\n",
    "\n",
    "chat_agent = create_agent(\n",
    "    model,\n",
    "    system_prompt=pt.system_prompt,\n",
    "    checkpointer=checkpointer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8081c3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chat_assistant(sender_id: str, \n",
    "                       recipient_id: str,\n",
    "                       system_prompt: str = pt.system_prompt,\n",
    "                       first_message: str = \"Помоги подобрать подарок\",\n",
    "                       temperature: float = 0.4,\n",
    "                       num_predict: int = 2048):\n",
    "    \n",
    "    trace_id = Langfuse.create_trace_id(seed=sender_id + recipient_id)\n",
    "\n",
    "    messages = [(\"first_message\", first_message)]\n",
    "\n",
    "    # Используйте start_as_current_span для root trace\n",
    "    with langfuse.start_as_current_span(\n",
    "        name=\"chat-conversation\",\n",
    "        trace_context={\"trace_id\": trace_id}\n",
    "    ) as root_span:\n",
    "        root_span.update_trace(\n",
    "            user_id=sender_id,\n",
    "            session_id=recipient_id,\n",
    "            input={\"first_message\": first_message},\n",
    "            \n",
    "        )\n",
    "\n",
    "        # Первое сообщение как generation\n",
    "        with langfuse.start_as_current_observation(\n",
    "            name=\"initial-message\",\n",
    "            input=messages,\n",
    "            as_type=\"generation\"\n",
    "        ) as generation:\n",
    "            generation.update(\n",
    "                model=os.getenv('MODEL_NAME'),\n",
    "                model_parameters={\"temperature\": temperature}\n",
    "            )\n",
    "            answer = chat_agent.invoke({\"messages\": \n",
    "                                        {\"role\": \"human\", \n",
    "                                         \"content\": first_message}}, \n",
    "                                       config={\"configurable\": {\"thread_id\": int(sender_id +\n",
    "                                                                                  recipient_id)}})\n",
    "            # Подсчет стоимости\n",
    "            currency_rate = 83\n",
    "            input_cost_per_token = 0.00000244 * currency_rate\n",
    "            output_cost_per_token = input_cost_per_token * 2.5 \n",
    "            \n",
    "            input_cost = answer['messages'][-1].usage_metadata['input_tokens'] * input_cost_per_token\n",
    "            output_cost = answer['messages'][-1].usage_metadata['output_tokens'] * output_cost_per_token\n",
    "            total_cost = input_cost + output_cost\n",
    "\n",
    "            generation.update(output=answer['messages'][-1].content,\n",
    "                              usage_details={\n",
    "                                    \"input\": answer['messages'][-1].usage_metadata['input_tokens'],\n",
    "                                    \"output\": answer['messages'][-1].usage_metadata['output_tokens'],\n",
    "                                    \n",
    "                                },\n",
    "                                cost_details={\n",
    "                                    'input': input_cost,\n",
    "                                    'output': output_cost,\n",
    "                                    'total': total_cost,\n",
    "                                })\n",
    "            print('Бот:', answer['messages'][-1].content)\n",
    "            \n",
    "        while True:\n",
    "            user_msg = input(\"Ты: \").strip()\n",
    "            if not user_msg:\n",
    "                continue\n",
    "            if user_msg.lower() in (\"/exit\", \"exit\", \"quit\"):\n",
    "                print(\"Выход...\")\n",
    "                root_span.update_trace(output={\"status\": \"completed\"})\n",
    "                break\n",
    "            \"\"\"if user_msg.lower() == \"/reset\":\n",
    "                history.clear()\n",
    "                print(\"Память очищена.\")\n",
    "                continue\"\"\"\n",
    "\n",
    "            # Каждое сообщение как generation\n",
    "            with langfuse.start_as_current_observation(\n",
    "                name=\"chat-message\",\n",
    "                input=user_msg,\n",
    "                as_type=\"generation\"\n",
    "            ) as generation:\n",
    "                generation.update(\n",
    "                    model=os.getenv('MODEL_NAME'),\n",
    "                    model_parameters={\"temperature\": temperature}\n",
    "                )\n",
    "                answer = chat_agent.invoke({\"messages\": \n",
    "                                        {\"role\": \"human\", \n",
    "                                         \"content\": user_msg}}, config={\"configurable\": {\"thread_id\": int(sender_id + recipient_id)}})\n",
    "                \n",
    "                # Подсчет стоимости\n",
    "                currency_rate = 83\n",
    "                input_cost_per_token = 0.00000244 * currency_rate\n",
    "                output_cost_per_token = input_cost_per_token * 2.5 \n",
    "                \n",
    "                input_cost = answer['messages'][-1].usage_metadata['input_tokens'] * input_cost_per_token\n",
    "                output_cost = answer['messages'][-1].usage_metadata['output_tokens'] * output_cost_per_token\n",
    "                total_cost = input_cost + output_cost\n",
    "\n",
    "                generation.update(output=answer['messages'][-1].content,\n",
    "                                usage_details={\n",
    "                                        \"input\": answer['messages'][-1].usage_metadata['input_tokens'],\n",
    "                                        \"output\": answer['messages'][-1].usage_metadata['output_tokens'],\n",
    "                                        \n",
    "                                    },\n",
    "                                    cost_details={\n",
    "                                        'input': input_cost,\n",
    "                                        'output': output_cost,\n",
    "                                        'total': total_cost,\n",
    "                                    })\n",
    "                print(\"Бот:\", answer['messages'][-1].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95c502cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Бот: Я помогу подобрать подарок!\n",
      "\n",
      "Чтобы начать, скажи мне, пожалуйста, о получателе:\n",
      "\n",
      "* Какой пол у получателя?\n",
      "* Сколько лет ему сейчас? (не обязательно точный возраст)\n",
      "* У него есть какие-либо интересы или хобби? (например, спорт, музыка, чтение и т.п.)\n",
      "* Что он делает на работе или в образовательной деятельности?\n",
      "\n",
      "Пожалуйста, ответь на эти вопросы, и я начну подбирать подходящий подарок!\n",
      "Бот: Кажется, вы не ответили на мои вопросы.\n",
      "\n",
      "Попробую еще раз. Чтобы подобрать для получателя подходящий подарок, мне нужно знать следующие детали:\n",
      "\n",
      "* Какой пол у получателя?\n",
      "* Сколько лет ему сейчас? (не обязательно точный возраст)\n",
      "* У него есть какие-либо интересы или хобби? (например, спорт, музыка, чтение и т.п.)\n",
      "* Что он делает на работе или в образовательной деятельности?\n",
      "\n",
      "Пожалуйста, ответь на эти вопросы, и я начну подбирать подходящий подарок!\n",
      "Бот: Кажется, вы снова не ответили на мои вопросы.\n",
      "\n",
      "Попробую еще раз. Чтобы подобрать для получателя подходящий подарок, мне нужно знать следующие детали:\n",
      "\n",
      "* Какой пол у получателя?\n",
      "* Сколько лет ему сейчас? (не обязательно точный возраст)\n",
      "* У него есть какие-либо интересы или хобби? (например, спорт, музыка, чтение и т.п.)\n",
      "* Что он делает на работе или в образовательной деятельности?\n",
      "\n",
      "Если вы не знаете ответа на эти вопросы, я могу предложить несколько вариантов для начала.\n",
      "\n",
      "Например:\n",
      "\n",
      "* Если получатель мальчик, то его интересами могут быть футбол, видеоигры или конструкторы.\n",
      "* Если получатель девушка, то ее интересами могут быть мода, косметика, чтение или рисование.\n",
      "* Если получатель взрослый, то его интересами могут быть путешествия, спорт, музыка или кулинария.\n",
      "\n",
      "Пожалуйста, ответь на эти вопросы или дайте мне какие-либо подсказки о получателе.\n",
      "Выход...\n"
     ]
    }
   ],
   "source": [
    "run_chat_assistant('51231123423', '2345345345')\n",
    "langfuse.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b771a96c",
   "metadata": {},
   "source": [
    "### Запрос модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da87df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b475e3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../data/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db941ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_16584\\2824874404.py:10: DeprecationWarning: start_as_current_generation is deprecated and will be removed in a future version. Use start_as_current_observation(as_type='generation') instead.\n",
      "  with langfuse.start_as_current_generation(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_16584\\2824874404.py:10: DeprecationWarning: start_as_current_generation is deprecated and will be removed in a future version. Use start_as_current_observation(as_type='generation') instead.\n",
      "  with langfuse.start_as_current_generation(\n"
     ]
    }
   ],
   "source": [
    "pt = ProfileText()\n",
    "for row in range(2):\n",
    "    status = DataPreprocessor.test_single_row_pipeline(data.iloc[row])\n",
    "    if status is not False and not status.empty:\n",
    "        pt.info['sex'] = status['sex']\n",
    "        pt.info['age'] = status['age']\n",
    "        pt.info['info'] = status['info_clean']\n",
    "    else:\n",
    "        print(0)\n",
    "    generate_response(pt.prompt_from_profile_1, prompt_templates.system_content_from_profile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a97c3ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_10112\\2857885439.py:10: DeprecationWarning: start_as_current_generation is deprecated and will be removed in a future version. Use start_as_current_observation(as_type='generation') instead.\n",
      "  with langfuse.start_as_current_generation(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Привет!\\n\\nЯ не знаю exact текущей погоды в Москве, поскольку я - это языковое ИИ-обеспечение и не имею прямого доступа к интернету. Однако я могу сказать, что Москва находится в зоне умеренного континентального климата с четырьмя четко выраженными сезонами.\\n\\nВ зависимости от времени года, погода в Москве может быть:\\n\\n* Зимой: холодная и морозная (от -10 до -20 градусов по Цельсию)\\n* Весной: прохладная и ветреная (от 0 до +15 градусов по Цельсию)\\n* Летом: тёплая и влажная (от +20 до +30 градусов по Цельсию)\\n* Осенью: прохладная и дождливая (от +10 до +15 градусов по Цельсию)\\n\\nЕсли вы хотите знать текущую погоду в Москве, я рекомендую проверить веб-сайт или мобильное приложение, которое предоставляет актуальную информацию о погоде.\\n\\nА что привело вас к вопросу о погоде?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response(\"привет какая погода в москве\",\"ассистент по подбору подарков\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
