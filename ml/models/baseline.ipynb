{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55eb02d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from langfuse import Langfuse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"C:/vuzik/sii/bachelor-2025-team-losoci/ml\"))\n",
    "\n",
    "import prompt_templates\n",
    "from prompt_templates import ProfileText\n",
    "from preprocessing_pipeline import DataPreprocessor\n",
    "\n",
    "from langfuse import Langfuse\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce40eacb",
   "metadata": {},
   "source": [
    "### Langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7906bd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse = Langfuse(\n",
    "        secret_key=os.getenv('LANGFUSE_SK'),\n",
    "        public_key=os.getenv('LANGFUSE_PK'),\n",
    "        host=os.getenv('LANGFUSE_HOST')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d665f2e8",
   "metadata": {},
   "source": [
    "#### Курс доллара\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12f58889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_currency_rate():\n",
    "    \n",
    "    url = \"https://api.exchangerate-api.com/v4/latest/USD\"\n",
    "    #try:\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    rub_rate = data['rates']['RUB']\n",
    "    return rub_rate\n",
    "    #except Exception as e:\n",
    "        #print(f\"Ошибка при получении курса: {e}\")\n",
    "        #return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df76bcb3",
   "metadata": {},
   "source": [
    "### Запросы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ebbeaf",
   "metadata": {},
   "source": [
    "#### По профилю"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4376c61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.checkpoint.memory import InMemorySaver  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61172521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt: str, \n",
    "                     system: str,\n",
    "                     model_name: str = None,\n",
    "                     max_tokens: int = 500,\n",
    "                     temperature: float = 0.7):\n",
    "                     \n",
    "    model = model_name or os.getenv('MODEL_NAME')\n",
    "    system_content = system\n",
    "\n",
    "    with langfuse.start_as_current_generation(\n",
    "        name=f\"Dataset test: pfp_1\",\n",
    "        model=model,\n",
    "        input={\n",
    "            \"system\": system_content,\n",
    "            \"prompt\": prompt},\n",
    "        model_parameters={\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens\n",
    "        }\n",
    "    ) as generation:\n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"system\": system_content,\n",
    "            \"stream\": False,\n",
    "            \"options\": {\n",
    "                \"temperature\": temperature,\n",
    "                \"max_tokens\": max_tokens\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        response = requests.post(os.getenv('BASE_URL'), json=data)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            full_response = response.json()\n",
    "            generated_text = full_response[\"response\"]\n",
    "\n",
    "            # Подсчет токенов\n",
    "            prompt_tokens = full_response.get(\"prompt_eval_count\", 0)\n",
    "            completion_tokens = full_response.get(\"eval_count\", 0)\n",
    "\n",
    "            # Подсчет стоимости токена из расчета\n",
    "            # rate = get_currency_rate()\n",
    "            currency_rate = 83\n",
    "            input_cost_per_token = 0.00000244 * currency_rate\n",
    "            output_cost_per_token = input_cost_per_token * 2.5 \n",
    "            \n",
    "            input_cost = prompt_tokens * input_cost_per_token\n",
    "            output_cost = completion_tokens * output_cost_per_token\n",
    "            total_cost = input_cost + output_cost\n",
    "\n",
    "            \n",
    "\n",
    "            generation.update(\n",
    "                output=generated_text,\n",
    "                usage_details={\n",
    "                    \"input\": prompt_tokens,\n",
    "                    \"output\": completion_tokens,\n",
    "                },\n",
    "                cost_details={\n",
    "                    \"input\": input_cost,\n",
    "                    \"output\": output_cost,\n",
    "                    \"total\": total_cost \n",
    "                },\n",
    "                metadata={\n",
    "                    \"response_time\": full_response.get(\"total_duration\", 0),\n",
    "                    \"model\": model,\n",
    "                    \"system\": system_content,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"stream\": False,\n",
    "                    #\"judge_response\": generated_judge_text,\n",
    "                    \"options\": {\n",
    "                        \"temperature\": temperature,\n",
    "                        \"max_tokens\": max_tokens\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "            )\n",
    "            \n",
    "            return generated_text\n",
    "            \n",
    "        else:\n",
    "            error_msg = f\"Ошибка API: {response.status_code} - {response.text}\"\n",
    "            \n",
    "            generation.update(\n",
    "                output={\"error\": error_msg},\n",
    "                level=\"ERROR\"\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": error_msg,\n",
    "                \"model\": model\n",
    "            }\n",
    "            \n",
    "    langfuse.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ff502f",
   "metadata": {},
   "source": [
    "#### Для локальной Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5437ed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_local_ollama(prompt: str, \n",
    "                     system: str,\n",
    "                     model_name: str = None,\n",
    "                     max_tokens: int = 500,\n",
    "                     temperature: float = 0.7):\n",
    "    \n",
    "    model = model_name or os.getenv('MODEL_NAME')\n",
    "    ollama_host = os.getenv('OLLAMA_HOST', 'http://localhost:11434')\n",
    "    \n",
    "    with langfuse.start_as_current_generation(\n",
    "        name=f\"Dataset test: pfp_1\",\n",
    "        model=model,\n",
    "        input={\n",
    "            \"system\": system,\n",
    "            \"prompt\": prompt},\n",
    "        model_parameters={\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens\n",
    "        }\n",
    "    ) as generation:\n",
    "        \n",
    "        try:\n",
    "            # Инициализация ChatOllama\n",
    "            llm = ChatOllama(\n",
    "                model=model,\n",
    "                temperature=temperature,\n",
    "                num_predict=max_tokens\n",
    "            )\n",
    "            \n",
    "            # Создание сообщений\n",
    "            messages = [\n",
    "                (\"system\", system),\n",
    "                (\"human\", prompt)\n",
    "            ]\n",
    "            \n",
    "            # Вызов модели\n",
    "            response = llm.invoke(messages) # response = requests.post(os.getenv('MODEL_NAME'), json=data)\n",
    "\n",
    "            generated_text = response.сcontent\n",
    "            \n",
    "            # Для получения точного количества токенов можно использовать дополнительный запрос\n",
    "            import requests\n",
    "            token_data = {\n",
    "                \"model\": model,\n",
    "                \"prompt\": f\"{system}\\n\\n{prompt}\"\n",
    "            }\n",
    "            \n",
    "            token_response = requests.post(f\"{ollama_host}/api/encode\", json=token_data)\n",
    "            if token_response.status_code == 200:\n",
    "                prompt_tokens = len(token_response.json().get(\"tokens\", []))\n",
    "            else:\n",
    "                prompt_tokens = len(prompt.split()) // 0.75\n",
    "                \n",
    "            completion_tokens = len(generated_text.split()) // 0.75\n",
    "            \n",
    "            # Подсчет стоимости\n",
    "            currency_rate = 83\n",
    "            input_cost_per_token = 0.00000244 * currency_rate\n",
    "            output_cost_per_token = input_cost_per_token * 2.5 \n",
    "            \n",
    "            input_cost = prompt_tokens * input_cost_per_token\n",
    "            output_cost = completion_tokens * output_cost_per_token\n",
    "            total_cost = input_cost + output_cost\n",
    "\n",
    "            generation.update(\n",
    "                output=generated_text,\n",
    "                usage_details={\n",
    "                    \"input\": prompt_tokens,\n",
    "                    \"output\": completion_tokens,\n",
    "                },\n",
    "                cost_details={\n",
    "                    \"input\": input_cost,\n",
    "                    \"output\": output_cost,\n",
    "                    \"total\": total_cost \n",
    "                },\n",
    "                metadata={\n",
    "                    \"model\": model,\n",
    "                    \"system\": system,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"temperature\": temperature,\n",
    "                    \"max_tokens\": max_tokens\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            return generated_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Ошибка ChatOllama: {str(e)}\"\n",
    "            \n",
    "            generation.update(\n",
    "                output={\"error\": error_msg},\n",
    "                level=\"ERROR\"\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": error_msg,\n",
    "                \"model\": model\n",
    "            }\n",
    "            \n",
    "    langfuse.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2353c17a",
   "metadata": {},
   "source": [
    "### Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76d9bf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.checkpoint.memory import InMemorySaver  \n",
    "from langfuse.langchain import CallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24cf0f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    validate_model_on_init=True,\n",
    "    temperature=0.4,\n",
    "    max_tokens=2048,\n",
    "    \n",
    ")\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "langfuse_handler = CallbackHandler()\n",
    "pt = ProfileText()\n",
    "\n",
    "chat_agent = create_agent(\n",
    "    model,\n",
    "    system_prompt=pt.system_prompt,\n",
    "    checkpointer=checkpointer,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ae3ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chat_assistant(prompt: str,\n",
    "                       sender_id: str, \n",
    "                       recipient_id: str,\n",
    "                       system_prompt: str = pt.system_prompt,\n",
    "                       temperature: float = 0.4,\n",
    "                       num_predict: int = 2048):\n",
    "    \n",
    "     answer = chat_agent.invoke({\"messages\": {\"role\": \"human\", \"content\": prompt}}, \n",
    "                                config={\"configurable\": {\"thread_id\": int(sender_id +recipient_id)},\n",
    "                                     \"callbacks\": [langfuse_handler]})\n",
    "     answer = answer['messages'][-1].content\n",
    "     return answer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d703d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    prompt: str\n",
    "    sender_id: str\n",
    "    recipient_id: str\n",
    "    system_prompt: Optional[str] = None\n",
    "    first_message: Optional[str] = \"Помоги подобрать подарок\"\n",
    "    temperature: Optional[float] = 0.4\n",
    "    num_predict: Optional[int] = 2048\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    answer: str\n",
    "    status: str = \"success\"\n",
    "\n",
    "@app.post(\"/chat\", response_model=ChatResponse)\n",
    "async def chat_endpoint(request: ChatRequest):\n",
    "    try:\n",
    "        answer = run_chat_assistant(\n",
    "            prompt=request.prompt,\n",
    "            sender_id=request.sender_id,\n",
    "            recipient_id=request.recipient_id,\n",
    "        )\n",
    "        \n",
    "        return ChatResponse(answer=answer)\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error: {str(e)}\")\n",
    "    \n",
    "uvicorn.run(\n",
    "        app, \n",
    "        host=\"26.205.227.135\",  # доступ с любого IP\n",
    "        port=8000,       # ← ваш порт\n",
    "        reload=True      # автоматическая перезагрузка при изменениях\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d492ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_chat_assistant(\"как дела\",'12','12')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4464bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chat_assistant(sender_id: str, \n",
    "                       recipient_id: str,\n",
    "                       system_prompt: str = pt.system_prompt,\n",
    "                       first_message: str = \"Помоги подобрать подарок\",\n",
    "                       temperature: float = 0.4,\n",
    "                       num_predict: int = 2048):\n",
    "    \n",
    "\n",
    "    messages = [(\"first_message\", first_message)]\n",
    "\n",
    "    answer = chat_agent.invoke({\"messages\": {\"role\": \"human\", \"content\": first_message}}, \n",
    "                                config={\"configurable\": {\"thread_id\": int(sender_id +recipient_id)},\n",
    "                                        \"callbacks\": [langfuse_handler]})\n",
    "            \n",
    "    print('Бот:', answer['messages'][-1].content)\n",
    "            \n",
    "    while True:\n",
    "        user_msg = input(\"Ты: \").strip()\n",
    "        if not user_msg:\n",
    "            continue\n",
    "        if user_msg.lower() in (\"/exit\", \"exit\", \"quit\"):\n",
    "            print(\"Выход...\")\n",
    "            break\n",
    "            \"\"\"if user_msg.lower() == \"/reset\":\n",
    "                history.clear()\n",
    "                print(\"Память очищена.\")\n",
    "                continue\"\"\"\n",
    "            \n",
    "        answer = chat_agent.invoke({\"messages\": {\"role\": \"human\", \"content\": user_msg}}, \n",
    "                               config={\"configurable\": {\"thread_id\": int(sender_id + recipient_id)},\n",
    "                                       'callbacks': [langfuse_handler]})\n",
    "        \n",
    "        print(\"Бот:\", answer['messages'][-1].content)\n",
    "        langfuse.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd4761f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrun_chat_assistant\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mкак дела\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m12\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m12\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mrun_chat_assistant\u001b[39m\u001b[34m(prompt, sender_id, recipient_id, system_prompt, first_message, temperature, num_predict)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_chat_assistant\u001b[39m(prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m      2\u001b[39m                        sender_id: \u001b[38;5;28mstr\u001b[39m, \n\u001b[32m      3\u001b[39m                        recipient_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m      \u001b[38;5;66;03m#                           config={\"configurable\": {\"thread_id\": int(sender_id +recipient_id)},\u001b[39;00m\n\u001b[32m     14\u001b[39m       \u001b[38;5;66;03m#                                  \"callbacks\": [langfuse_handler]})\u001b[39;00m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chat_agent.stream({\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: prompt}}, \n\u001b[32m     17\u001b[39m                                config={\u001b[33m\"\u001b[39m\u001b[33mconfigurable\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mthread_id\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mint\u001b[39m(sender_id +recipient_id)},\n\u001b[32m     18\u001b[39m                                       \u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: [langfuse_handler]}):\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m, end=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'text'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b771a96c",
   "metadata": {},
   "source": [
    "### Запрос модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da87df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b475e3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../data/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db941ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = ProfileText()\n",
    "for row in range(2):\n",
    "    status = DataPreprocessor.test_single_row_pipeline(data.iloc[row])\n",
    "    if status is not False and not status.empty:\n",
    "        pt.info['sex'] = status['sex']\n",
    "        pt.info['age'] = status['age']\n",
    "        pt.info['info'] = status['info_clean']\n",
    "    else:\n",
    "        print(0)\n",
    "    generate_response(pt.prompt_from_profile_1, prompt_templates.system_content_from_profile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97c3ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_response(\"привет какая погода в москве\",\"ассистент по подбору подарков\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
