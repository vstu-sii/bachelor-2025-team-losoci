{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55eb02d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langfuse import Langfuse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"C:/вузик/bachelor-2025-team-losoci/ml\"))\n",
    "\n",
    "import prompt_templates\n",
    "from prompt_templates import ProfileText\n",
    "from preprocessing_pipeline import DataPreprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c731bd8",
   "metadata": {},
   "source": [
    "### Параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01e6e952",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG = {\n",
    "    \"model_name\": \"llama3.1\",\n",
    "    \"base_url\": \"http://26.81.6.105:11434/api/generate\",\n",
    "    \"langfuse_secret_key\": \"sk-lf-03128ca3-f1c4-4e1b-ba7c-17b8cf50f892\",\n",
    "    \"langfuse_public_key\": \"pk-lf-76c5e16f-7acd-420f-9e29-6d0f59507b4a\", \n",
    "    \"langfuse_host\": \"http://26.81.6.105:3000\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7906bd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse = Langfuse(\n",
    "    secret_key=MODEL_CONFIG[\"langfuse_secret_key\"],\n",
    "    public_key=MODEL_CONFIG[\"langfuse_public_key\"],\n",
    "    host=MODEL_CONFIG[\"langfuse_host\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2f5bdf",
   "metadata": {},
   "source": [
    "### Функции"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d665f2e8",
   "metadata": {},
   "source": [
    "#### Курс доллара"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12f58889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_currency_rate():\n",
    "    \n",
    "    url = \"https://api.exchangerate-api.com/v4/latest/USD\"\n",
    "    #try:\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    rub_rate = data['rates']['RUB']\n",
    "    return rub_rate\n",
    "    #except Exception as e:\n",
    "        #print(f\"Ошибка при получении курса: {e}\")\n",
    "        #return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ebbeaf",
   "metadata": {},
   "source": [
    "#### Отправка запроса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61172521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt: str, \n",
    "                     system: str,\n",
    "                     model_name: str = None,\n",
    "                     max_tokens: int = 500,\n",
    "                     temperature: float = 0.7):\n",
    "                     \n",
    "    model = model_name or MODEL_CONFIG[\"model_name\"]\n",
    "    system_content = system\n",
    "\n",
    "    with langfuse.start_as_current_generation(\n",
    "        name=f\"Dataset test: pfp_1\",\n",
    "        model=model,\n",
    "        input={\n",
    "            \"system\": system_content,\n",
    "            \"prompt\": prompt},\n",
    "        model_parameters={\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens\n",
    "        }\n",
    "    ) as generation:\n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"system\": system_content,\n",
    "            \"stream\": False,\n",
    "            \"options\": {\n",
    "                \"temperature\": temperature,\n",
    "                \"max_tokens\": max_tokens\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        response = requests.post(MODEL_CONFIG[\"base_url\"], json=data)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            full_response = response.json()\n",
    "            generated_text = full_response[\"response\"]\n",
    "\n",
    "            # Подсчет токенов\n",
    "            prompt_tokens = full_response.get(\"prompt_eval_count\", 0)\n",
    "            completion_tokens = full_response.get(\"eval_count\", 0)\n",
    "\n",
    "            # Подсчет стоимости токена из расчета\n",
    "            # rate = get_currency_rate()\n",
    "            currency_rate = 83\n",
    "            input_cost_per_token = 0.00000244 * currency_rate\n",
    "            output_cost_per_token = input_cost_per_token * 2.5 \n",
    "            \n",
    "            input_cost = prompt_tokens * input_cost_per_token\n",
    "            output_cost = completion_tokens * output_cost_per_token\n",
    "            total_cost = input_cost + output_cost\n",
    "\n",
    "            judge_response = requests.post(MODEL_CONFIG[\"base_url\"], \n",
    "                                           json={\"model\": model,\n",
    "                                                \"prompt\": f\"\"\"system : {system};\n",
    "                                                            prompt : {prompt};\n",
    "                                                            формат ответа:\n",
    "                                                            Номер, название подарка (максимум 5 слов описание), релевантность от 1 до 5\"\"\",\n",
    "                                                \"system\": \"Ты должен оценить релевантность подарка от 1 до 5 исходя из промпта\",\n",
    "                                                \"stream\": False,\n",
    "                                                \"options\": {\n",
    "                                                    \"temperature\": temperature,\n",
    "                                                    \"max_tokens\": max_tokens\n",
    "                                                }\n",
    "                \n",
    "            })\n",
    "            full_judge_response = judge_response.json()\n",
    "            generated_judge_text = full_judge_response[\"response\"]\n",
    "\n",
    "            generation.update(\n",
    "                output=generated_text,\n",
    "                usage_details={\n",
    "                    \"input\": prompt_tokens,\n",
    "                    \"output\": completion_tokens,\n",
    "                },\n",
    "                cost_details={\n",
    "                    \"input\": input_cost,\n",
    "                    \"output\": output_cost,\n",
    "                    \"total\": total_cost \n",
    "                },\n",
    "                metadata={\n",
    "                    \"response_time\": full_response.get(\"total_duration\", 0),\n",
    "                    \"model\": model,\n",
    "                    \"system\": system_content,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"stream\": False,\n",
    "                    \"judge_response\": generated_judge_text,\n",
    "                    \"options\": {\n",
    "                        \"temperature\": temperature,\n",
    "                        \"max_tokens\": max_tokens\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "            )\n",
    "            \n",
    "            return generated_text\n",
    "            \n",
    "        else:\n",
    "            error_msg = f\"Ошибка API: {response.status_code} - {response.text}\"\n",
    "            \n",
    "            generation.update(\n",
    "                output={\"error\": error_msg},\n",
    "                level=\"ERROR\"\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": error_msg,\n",
    "                \"model\": model\n",
    "            }\n",
    "            \n",
    "    langfuse.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b771a96c",
   "metadata": {},
   "source": [
    "### Запрос модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5da87df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b475e3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../data/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9db941ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Егор\\AppData\\Local\\Temp\\ipykernel_5204\\1782826480.py:10: DeprecationWarning: start_as_current_generation is deprecated and will be removed in a future version. Use start_as_current_observation(as_type='generation') instead.\n",
      "  with langfuse.start_as_current_generation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Егор\\AppData\\Local\\Temp\\ipykernel_5204\\1782826480.py:10: DeprecationWarning: start_as_current_generation is deprecated and will be removed in a future version. Use start_as_current_observation(as_type='generation') instead.\n",
      "  with langfuse.start_as_current_generation(\n"
     ]
    }
   ],
   "source": [
    "pt = ProfileText()\n",
    "for row in range(2):\n",
    "    status = DataPreprocessor.test_single_row_pipeline(data.iloc[row])\n",
    "    if status is not False and not status.empty:\n",
    "        pt.info['sex'] = status['sex']\n",
    "        pt.info['age'] = status['age']\n",
    "        pt.info['info'] = status['info_clean']\n",
    "    else:\n",
    "        print(0)\n",
    "    generate_response(pt.prompt_from_profile_1, prompt_templates.system_content_from_profile)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
